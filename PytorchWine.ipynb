{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PytorchWine.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/atadria/AGH_data_science/blob/master/PytorchWine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "3_vyrKoI6fZp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Pytorch project \n",
        "\n",
        "##Using chemical analysis determine the origin of wines\n",
        "\n",
        "https://archive.ics.uci.edu/ml/datasets/wine\n"
      ]
    },
    {
      "metadata": {
        "id": "qDKiWD6v6dd4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Imports"
      ]
    },
    {
      "metadata": {
        "id": "ieaVI4Kg7Mos",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# http://pytorch.org/\n",
        "from os.path import exists\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2UZTS8wA8H7J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rgDEZFJ_8Mvx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Read data from csv\n",
        "https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data"
      ]
    },
    {
      "metadata": {
        "id": "hG9MszWb8LDa",
        "colab_type": "code",
        "outputId": "5a19b66d-f42b-4b3d-dba1-29a39638a110",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        }
      },
      "cell_type": "code",
      "source": [
        "data_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data'\n",
        "wine_data = pd.read_csv(data_url, header=None)\n",
        "train_data = wine_data.sample(frac=0.8)\n",
        "test_data = wine_data.drop(train_data.index)\n",
        "train_classes = train_data[0]\n",
        "train_data = train_data.drop(0, axis = 1)\n",
        "test_classes = test_data[0]\n",
        "test_data = test_data.drop(0, axis = 1)\n",
        "test_data.head(10)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>13.16</td>\n",
              "      <td>2.36</td>\n",
              "      <td>2.67</td>\n",
              "      <td>18.6</td>\n",
              "      <td>101</td>\n",
              "      <td>2.80</td>\n",
              "      <td>3.24</td>\n",
              "      <td>0.30</td>\n",
              "      <td>2.81</td>\n",
              "      <td>5.68</td>\n",
              "      <td>1.03</td>\n",
              "      <td>3.17</td>\n",
              "      <td>1185</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>14.06</td>\n",
              "      <td>2.15</td>\n",
              "      <td>2.61</td>\n",
              "      <td>17.6</td>\n",
              "      <td>121</td>\n",
              "      <td>2.60</td>\n",
              "      <td>2.51</td>\n",
              "      <td>0.31</td>\n",
              "      <td>1.25</td>\n",
              "      <td>5.05</td>\n",
              "      <td>1.06</td>\n",
              "      <td>3.58</td>\n",
              "      <td>1295</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>13.86</td>\n",
              "      <td>1.35</td>\n",
              "      <td>2.27</td>\n",
              "      <td>16.0</td>\n",
              "      <td>98</td>\n",
              "      <td>2.98</td>\n",
              "      <td>3.15</td>\n",
              "      <td>0.22</td>\n",
              "      <td>1.85</td>\n",
              "      <td>7.22</td>\n",
              "      <td>1.01</td>\n",
              "      <td>3.55</td>\n",
              "      <td>1045</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>12.93</td>\n",
              "      <td>3.80</td>\n",
              "      <td>2.65</td>\n",
              "      <td>18.6</td>\n",
              "      <td>102</td>\n",
              "      <td>2.41</td>\n",
              "      <td>2.41</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.98</td>\n",
              "      <td>4.50</td>\n",
              "      <td>1.03</td>\n",
              "      <td>3.52</td>\n",
              "      <td>770</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>13.87</td>\n",
              "      <td>1.90</td>\n",
              "      <td>2.80</td>\n",
              "      <td>19.4</td>\n",
              "      <td>107</td>\n",
              "      <td>2.95</td>\n",
              "      <td>2.97</td>\n",
              "      <td>0.37</td>\n",
              "      <td>1.76</td>\n",
              "      <td>4.50</td>\n",
              "      <td>1.25</td>\n",
              "      <td>3.40</td>\n",
              "      <td>915</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>13.73</td>\n",
              "      <td>1.50</td>\n",
              "      <td>2.70</td>\n",
              "      <td>22.5</td>\n",
              "      <td>101</td>\n",
              "      <td>3.00</td>\n",
              "      <td>3.25</td>\n",
              "      <td>0.29</td>\n",
              "      <td>2.38</td>\n",
              "      <td>5.70</td>\n",
              "      <td>1.19</td>\n",
              "      <td>2.71</td>\n",
              "      <td>1285</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>13.76</td>\n",
              "      <td>1.53</td>\n",
              "      <td>2.70</td>\n",
              "      <td>19.5</td>\n",
              "      <td>132</td>\n",
              "      <td>2.95</td>\n",
              "      <td>2.74</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.35</td>\n",
              "      <td>5.40</td>\n",
              "      <td>1.25</td>\n",
              "      <td>3.00</td>\n",
              "      <td>1235</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>13.05</td>\n",
              "      <td>1.65</td>\n",
              "      <td>2.55</td>\n",
              "      <td>18.0</td>\n",
              "      <td>98</td>\n",
              "      <td>2.45</td>\n",
              "      <td>2.43</td>\n",
              "      <td>0.29</td>\n",
              "      <td>1.44</td>\n",
              "      <td>4.25</td>\n",
              "      <td>1.12</td>\n",
              "      <td>2.51</td>\n",
              "      <td>1105</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>14.21</td>\n",
              "      <td>4.04</td>\n",
              "      <td>2.44</td>\n",
              "      <td>18.9</td>\n",
              "      <td>111</td>\n",
              "      <td>2.85</td>\n",
              "      <td>2.65</td>\n",
              "      <td>0.30</td>\n",
              "      <td>1.25</td>\n",
              "      <td>5.24</td>\n",
              "      <td>0.87</td>\n",
              "      <td>3.33</td>\n",
              "      <td>1080</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>13.94</td>\n",
              "      <td>1.73</td>\n",
              "      <td>2.27</td>\n",
              "      <td>17.4</td>\n",
              "      <td>108</td>\n",
              "      <td>2.88</td>\n",
              "      <td>3.54</td>\n",
              "      <td>0.32</td>\n",
              "      <td>2.08</td>\n",
              "      <td>8.90</td>\n",
              "      <td>1.12</td>\n",
              "      <td>3.10</td>\n",
              "      <td>1260</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       1     2     3     4    5     6     7     8     9     10    11    12  \\\n",
              "2   13.16  2.36  2.67  18.6  101  2.80  3.24  0.30  2.81  5.68  1.03  3.17   \n",
              "7   14.06  2.15  2.61  17.6  121  2.60  2.51  0.31  1.25  5.05  1.06  3.58   \n",
              "9   13.86  1.35  2.27  16.0   98  2.98  3.15  0.22  1.85  7.22  1.01  3.55   \n",
              "21  12.93  3.80  2.65  18.6  102  2.41  2.41  0.25  1.98  4.50  1.03  3.52   \n",
              "28  13.87  1.90  2.80  19.4  107  2.95  2.97  0.37  1.76  4.50  1.25  3.40   \n",
              "30  13.73  1.50  2.70  22.5  101  3.00  3.25  0.29  2.38  5.70  1.19  2.71   \n",
              "33  13.76  1.53  2.70  19.5  132  2.95  2.74  0.50  1.35  5.40  1.25  3.00   \n",
              "37  13.05  1.65  2.55  18.0   98  2.45  2.43  0.29  1.44  4.25  1.12  2.51   \n",
              "45  14.21  4.04  2.44  18.9  111  2.85  2.65  0.30  1.25  5.24  0.87  3.33   \n",
              "49  13.94  1.73  2.27  17.4  108  2.88  3.54  0.32  2.08  8.90  1.12  3.10   \n",
              "\n",
              "      13  \n",
              "2   1185  \n",
              "7   1295  \n",
              "9   1045  \n",
              "21   770  \n",
              "28   915  \n",
              "30  1285  \n",
              "33  1235  \n",
              "37  1105  \n",
              "45  1080  \n",
              "49  1260  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "metadata": {
        "id": "g6FubGzp-zsz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Normalize data"
      ]
    },
    {
      "metadata": {
        "id": "n9VLotNoADQh",
        "colab_type": "code",
        "outputId": "6e5c41dd-ff54-4b59-cfaa-b99685f956f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        }
      },
      "cell_type": "code",
      "source": [
        "train_data = train_data.apply(lambda x: 10*(x - x.min()) / (x.max() - x.min()))\n",
        "test_data = test_data.apply(lambda x: 10*(x - x.min()) / (x.max() - x.min()))\n",
        "test_data.head(10)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5.372549</td>\n",
              "      <td>4.068768</td>\n",
              "      <td>8.522727</td>\n",
              "      <td>2.08</td>\n",
              "      <td>2.738095</td>\n",
              "      <td>7.021277</td>\n",
              "      <td>9.016393</td>\n",
              "      <td>2.500000</td>\n",
              "      <td>8.356643</td>\n",
              "      <td>3.345455</td>\n",
              "      <td>5.925926</td>\n",
              "      <td>8.209607</td>\n",
              "      <td>8.905473</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>8.901961</td>\n",
              "      <td>3.467049</td>\n",
              "      <td>7.840909</td>\n",
              "      <td>1.28</td>\n",
              "      <td>5.119048</td>\n",
              "      <td>6.170213</td>\n",
              "      <td>6.622951</td>\n",
              "      <td>2.727273</td>\n",
              "      <td>2.902098</td>\n",
              "      <td>2.772727</td>\n",
              "      <td>6.296296</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>8.117647</td>\n",
              "      <td>1.174785</td>\n",
              "      <td>3.977273</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2.380952</td>\n",
              "      <td>7.787234</td>\n",
              "      <td>8.721311</td>\n",
              "      <td>0.681818</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>4.745455</td>\n",
              "      <td>5.679012</td>\n",
              "      <td>9.868996</td>\n",
              "      <td>7.512438</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>4.470588</td>\n",
              "      <td>8.194842</td>\n",
              "      <td>8.295455</td>\n",
              "      <td>2.08</td>\n",
              "      <td>2.857143</td>\n",
              "      <td>5.361702</td>\n",
              "      <td>6.295082</td>\n",
              "      <td>1.363636</td>\n",
              "      <td>5.454545</td>\n",
              "      <td>2.272727</td>\n",
              "      <td>5.925926</td>\n",
              "      <td>9.737991</td>\n",
              "      <td>4.776119</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>8.156863</td>\n",
              "      <td>2.750716</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>2.72</td>\n",
              "      <td>3.452381</td>\n",
              "      <td>7.659574</td>\n",
              "      <td>8.131148</td>\n",
              "      <td>4.090909</td>\n",
              "      <td>4.685315</td>\n",
              "      <td>2.272727</td>\n",
              "      <td>8.641975</td>\n",
              "      <td>9.213974</td>\n",
              "      <td>6.218905</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>7.607843</td>\n",
              "      <td>1.604585</td>\n",
              "      <td>8.863636</td>\n",
              "      <td>5.20</td>\n",
              "      <td>2.738095</td>\n",
              "      <td>7.872340</td>\n",
              "      <td>9.049180</td>\n",
              "      <td>2.272727</td>\n",
              "      <td>6.853147</td>\n",
              "      <td>3.363636</td>\n",
              "      <td>7.901235</td>\n",
              "      <td>6.200873</td>\n",
              "      <td>9.900498</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>7.725490</td>\n",
              "      <td>1.690544</td>\n",
              "      <td>8.863636</td>\n",
              "      <td>2.80</td>\n",
              "      <td>6.428571</td>\n",
              "      <td>7.659574</td>\n",
              "      <td>7.377049</td>\n",
              "      <td>7.045455</td>\n",
              "      <td>3.251748</td>\n",
              "      <td>3.090909</td>\n",
              "      <td>8.641975</td>\n",
              "      <td>7.467249</td>\n",
              "      <td>9.402985</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>4.941176</td>\n",
              "      <td>2.034384</td>\n",
              "      <td>7.159091</td>\n",
              "      <td>1.60</td>\n",
              "      <td>2.380952</td>\n",
              "      <td>5.531915</td>\n",
              "      <td>6.360656</td>\n",
              "      <td>2.272727</td>\n",
              "      <td>3.566434</td>\n",
              "      <td>2.045455</td>\n",
              "      <td>7.037037</td>\n",
              "      <td>5.327511</td>\n",
              "      <td>8.109453</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>9.490196</td>\n",
              "      <td>8.882521</td>\n",
              "      <td>5.909091</td>\n",
              "      <td>2.32</td>\n",
              "      <td>3.928571</td>\n",
              "      <td>7.234043</td>\n",
              "      <td>7.081967</td>\n",
              "      <td>2.500000</td>\n",
              "      <td>2.902098</td>\n",
              "      <td>2.945455</td>\n",
              "      <td>3.950617</td>\n",
              "      <td>8.908297</td>\n",
              "      <td>7.860697</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>8.431373</td>\n",
              "      <td>2.263610</td>\n",
              "      <td>3.977273</td>\n",
              "      <td>1.12</td>\n",
              "      <td>3.571429</td>\n",
              "      <td>7.361702</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>2.954545</td>\n",
              "      <td>5.804196</td>\n",
              "      <td>6.272727</td>\n",
              "      <td>7.037037</td>\n",
              "      <td>7.903930</td>\n",
              "      <td>9.651741</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          1         2          3     4         5         6          7   \\\n",
              "2   5.372549  4.068768   8.522727  2.08  2.738095  7.021277   9.016393   \n",
              "7   8.901961  3.467049   7.840909  1.28  5.119048  6.170213   6.622951   \n",
              "9   8.117647  1.174785   3.977273  0.00  2.380952  7.787234   8.721311   \n",
              "21  4.470588  8.194842   8.295455  2.08  2.857143  5.361702   6.295082   \n",
              "28  8.156863  2.750716  10.000000  2.72  3.452381  7.659574   8.131148   \n",
              "30  7.607843  1.604585   8.863636  5.20  2.738095  7.872340   9.049180   \n",
              "33  7.725490  1.690544   8.863636  2.80  6.428571  7.659574   7.377049   \n",
              "37  4.941176  2.034384   7.159091  1.60  2.380952  5.531915   6.360656   \n",
              "45  9.490196  8.882521   5.909091  2.32  3.928571  7.234043   7.081967   \n",
              "49  8.431373  2.263610   3.977273  1.12  3.571429  7.361702  10.000000   \n",
              "\n",
              "          8         9         10        11         12         13  \n",
              "2   2.500000  8.356643  3.345455  5.925926   8.209607   8.905473  \n",
              "7   2.727273  2.902098  2.772727  6.296296  10.000000  10.000000  \n",
              "9   0.681818  5.000000  4.745455  5.679012   9.868996   7.512438  \n",
              "21  1.363636  5.454545  2.272727  5.925926   9.737991   4.776119  \n",
              "28  4.090909  4.685315  2.272727  8.641975   9.213974   6.218905  \n",
              "30  2.272727  6.853147  3.363636  7.901235   6.200873   9.900498  \n",
              "33  7.045455  3.251748  3.090909  8.641975   7.467249   9.402985  \n",
              "37  2.272727  3.566434  2.045455  7.037037   5.327511   8.109453  \n",
              "45  2.500000  2.902098  2.945455  3.950617   8.908297   7.860697  \n",
              "49  2.954545  5.804196  6.272727  7.037037   7.903930   9.651741  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "metadata": {
        "id": "N4qvVRGkEYcl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Load data"
      ]
    },
    {
      "metadata": {
        "id": "qUQC47dDHWCa",
        "colab_type": "code",
        "outputId": "b7dbaa9d-3da7-4def-ad93-885051a96685",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "train_set = torch.utils.data.TensorDataset(torch.tensor(train_data.values).float(), torch.tensor(train_classes.values - 1))\n",
        "train_dataloader = torch.utils.data.DataLoader(train_set, batch_size=32, shuffle=True)\n",
        "\n",
        "test_set = torch.utils.data.TensorDataset(torch.tensor(test_data.values).float(), torch.tensor(test_classes.values - 1))\n",
        "test_dataloader = torch.utils.data.DataLoader(test_set, batch_size=128)\n",
        "\n",
        "torch.tensor(train_classes.values -1)"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([2, 0, 1, 2, 1, 1, 2, 0, 0, 1, 2, 0, 0, 2, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,\n",
              "        1, 1, 1, 2, 2, 0, 2, 1, 2, 1, 0, 2, 0, 0, 1, 2, 1, 0, 0, 1, 1, 2, 1, 1,\n",
              "        0, 2, 2, 2, 1, 1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 2, 1, 1, 1, 0, 0, 0, 1,\n",
              "        1, 0, 2, 0, 2, 1, 1, 2, 1, 1, 1, 1, 1, 2, 2, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
              "        1, 1, 0, 0, 1, 2, 1, 2, 0, 0, 0, 0, 0, 1, 1, 2, 1, 2, 0, 0, 0, 0, 2, 0,\n",
              "        1, 1, 1, 1, 1, 1, 0, 1, 1, 2, 2, 1, 0, 2, 0, 2, 2, 2, 0, 0, 1, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "metadata": {
        "id": "OadXe-xpPCMz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Define model"
      ]
    },
    {
      "metadata": {
        "id": "dn_ozHbwPFW-",
        "colab_type": "code",
        "outputId": "5e695627-1346-4cf0-afe6-a0f756abe335",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # linear layer (13 -> 20)\n",
        "        self.fc1 = nn.Linear(13, 15)\n",
        "        # linear layer (500 -> 10)\n",
        "        self.fc2 = nn.Linear(15, 3)\n",
        "        # dropout layer (p=0.25)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "        # add 1st hidden layer, with relu activation function\n",
        "        x = F.relu(self.fc1(x))\n",
        "        # add dropout layer\n",
        "        x = self.dropout(x)\n",
        "        # add 2nd hidden layer, with relu activation function\n",
        "        x = F.relu(self.fc2(x))\n",
        "        #x = F.softmax(x, dim=1)\n",
        "        return x\n",
        "\n",
        "model = Net()\n",
        "print(model)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  model.cuda()"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (fc1): Linear(in_features=13, out_features=15, bias=True)\n",
            "  (fc2): Linear(in_features=15, out_features=3, bias=True)\n",
            "  (dropout): Dropout(p=0.1)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "y-Ueje09QtC6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# specify loss function (categorical cross-entropy)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# specify optimizer\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 100 ,gamma=0.9, last_epoch=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HzWA00-iQwjd",
        "colab_type": "code",
        "outputId": "dd1c4789-8081-494e-8aa6-54b7049252b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 8398
        }
      },
      "cell_type": "code",
      "source": [
        "# number of epochs to train the model\n",
        "n_epochs = 400\n",
        "valid_loss_min = np.Inf # track change in validation loss\n",
        "\n",
        "for epoch in range(1, n_epochs+1):\n",
        "\n",
        "    # keep track of training and validation loss\n",
        "    train_loss = 0.0\n",
        "    valid_loss = 0.0\n",
        "    \n",
        "    ###################\n",
        "    # train the model #\n",
        "    ###################\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_dataloader):\n",
        "        # move tensors to GPU if CUDA is available\n",
        "        if torch.cuda.is_available():\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "        # clear the gradients of all optimized variables\n",
        "        optimizer.zero_grad()\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        output = model(data)\n",
        "        # calculate the batch loss\n",
        "        loss = criterion(output, target)\n",
        "        # backward pass: compute gradient of the loss with respect to model parameters\n",
        "        loss.backward()\n",
        "        # perform a single optimization step (parameter update)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        # update training loss\n",
        "        train_loss += loss.item()*data.size(0)    \n",
        "    ######################    \n",
        "    # validate the model #\n",
        "    ######################\n",
        "    model.eval()\n",
        "    accuracy = 0\n",
        "    with torch.no_grad():\n",
        "      for batch_idx, (data, target) in enumerate(test_dataloader):\n",
        "          # move tensors to GPU if CUDA is available\n",
        "          if torch.cuda.is_available():\n",
        "              data, target = data.cuda(), target.cuda()\n",
        "          # forward pass: compute predicted outputs by passing inputs to the model\n",
        "          output = model(data)\n",
        "          # calculate the batch loss\n",
        "          loss = criterion(output, target)\n",
        "          # update average validation loss \n",
        "          valid_loss += loss.item()*data.size(0)\n",
        "          #calculate accuracy \n",
        "          ps = torch.exp(model.forward(data))\n",
        "          top_p, top_class = ps.topk(1, dim=1)\n",
        "          equals = top_class == target.view(*top_class.shape)\n",
        "          accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
        "    \n",
        "    # calculate average losses\n",
        "    train_loss = train_loss/len(train_dataloader.dataset)\n",
        "    valid_loss = valid_loss/len(test_dataloader.dataset)\n",
        "    accuracy = accuracy/len(test_dataloader) \n",
        "   \n",
        "    # print training/validation statistics \n",
        "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f} \\tAccuracy: {:.6f}'.format(\n",
        "        epoch, train_loss, valid_loss, accuracy))\n",
        "    \n",
        "    # save model if validation loss has decreased\n",
        "    if valid_loss <= valid_loss_min:\n",
        "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
        "        valid_loss_min,\n",
        "        valid_loss))\n",
        "        torch.save(model.state_dict(), 'model_augmented.pt')\n",
        "        valid_loss_min = valid_loss"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1 \tTraining Loss: 1.204040 \tValidation Loss: 1.139395 \tAccuracy: 0.305556\n",
            "Validation loss decreased (inf --> 1.139395).  Saving model ...\n",
            "Epoch: 2 \tTraining Loss: 1.099403 \tValidation Loss: 1.092857 \tAccuracy: 0.416667\n",
            "Validation loss decreased (1.139395 --> 1.092857).  Saving model ...\n",
            "Epoch: 3 \tTraining Loss: 1.039211 \tValidation Loss: 1.053691 \tAccuracy: 0.500000\n",
            "Validation loss decreased (1.092857 --> 1.053691).  Saving model ...\n",
            "Epoch: 4 \tTraining Loss: 0.985814 \tValidation Loss: 1.024930 \tAccuracy: 0.500000\n",
            "Validation loss decreased (1.053691 --> 1.024930).  Saving model ...\n",
            "Epoch: 5 \tTraining Loss: 0.946845 \tValidation Loss: 0.982868 \tAccuracy: 0.583333\n",
            "Validation loss decreased (1.024930 --> 0.982868).  Saving model ...\n",
            "Epoch: 6 \tTraining Loss: 0.908342 \tValidation Loss: 0.945257 \tAccuracy: 0.555556\n",
            "Validation loss decreased (0.982868 --> 0.945257).  Saving model ...\n",
            "Epoch: 7 \tTraining Loss: 0.870216 \tValidation Loss: 0.907373 \tAccuracy: 0.583333\n",
            "Validation loss decreased (0.945257 --> 0.907373).  Saving model ...\n",
            "Epoch: 8 \tTraining Loss: 0.854212 \tValidation Loss: 0.874061 \tAccuracy: 0.611111\n",
            "Validation loss decreased (0.907373 --> 0.874061).  Saving model ...\n",
            "Epoch: 9 \tTraining Loss: 0.792012 \tValidation Loss: 0.840892 \tAccuracy: 0.666667\n",
            "Validation loss decreased (0.874061 --> 0.840892).  Saving model ...\n",
            "Epoch: 10 \tTraining Loss: 0.756854 \tValidation Loss: 0.804475 \tAccuracy: 0.694444\n",
            "Validation loss decreased (0.840892 --> 0.804475).  Saving model ...\n",
            "Epoch: 11 \tTraining Loss: 0.751539 \tValidation Loss: 0.769974 \tAccuracy: 0.722222\n",
            "Validation loss decreased (0.804475 --> 0.769974).  Saving model ...\n",
            "Epoch: 12 \tTraining Loss: 0.670359 \tValidation Loss: 0.734501 \tAccuracy: 0.694444\n",
            "Validation loss decreased (0.769974 --> 0.734501).  Saving model ...\n",
            "Epoch: 13 \tTraining Loss: 0.656110 \tValidation Loss: 0.699191 \tAccuracy: 0.750000\n",
            "Validation loss decreased (0.734501 --> 0.699191).  Saving model ...\n",
            "Epoch: 14 \tTraining Loss: 0.647898 \tValidation Loss: 0.681989 \tAccuracy: 0.777778\n",
            "Validation loss decreased (0.699191 --> 0.681989).  Saving model ...\n",
            "Epoch: 15 \tTraining Loss: 0.611591 \tValidation Loss: 0.649796 \tAccuracy: 0.805556\n",
            "Validation loss decreased (0.681989 --> 0.649796).  Saving model ...\n",
            "Epoch: 16 \tTraining Loss: 0.596254 \tValidation Loss: 0.635262 \tAccuracy: 0.805556\n",
            "Validation loss decreased (0.649796 --> 0.635262).  Saving model ...\n",
            "Epoch: 17 \tTraining Loss: 0.600157 \tValidation Loss: 0.607200 \tAccuracy: 0.833333\n",
            "Validation loss decreased (0.635262 --> 0.607200).  Saving model ...\n",
            "Epoch: 18 \tTraining Loss: 0.546102 \tValidation Loss: 0.574803 \tAccuracy: 0.805556\n",
            "Validation loss decreased (0.607200 --> 0.574803).  Saving model ...\n",
            "Epoch: 19 \tTraining Loss: 0.527027 \tValidation Loss: 0.544019 \tAccuracy: 0.833333\n",
            "Validation loss decreased (0.574803 --> 0.544019).  Saving model ...\n",
            "Epoch: 20 \tTraining Loss: 0.469353 \tValidation Loss: 0.526811 \tAccuracy: 0.833333\n",
            "Validation loss decreased (0.544019 --> 0.526811).  Saving model ...\n",
            "Epoch: 21 \tTraining Loss: 0.460294 \tValidation Loss: 0.528598 \tAccuracy: 0.833333\n",
            "Epoch: 22 \tTraining Loss: 0.426939 \tValidation Loss: 0.503399 \tAccuracy: 0.833333\n",
            "Validation loss decreased (0.526811 --> 0.503399).  Saving model ...\n",
            "Epoch: 23 \tTraining Loss: 0.430516 \tValidation Loss: 0.480094 \tAccuracy: 0.833333\n",
            "Validation loss decreased (0.503399 --> 0.480094).  Saving model ...\n",
            "Epoch: 24 \tTraining Loss: 0.450314 \tValidation Loss: 0.478852 \tAccuracy: 0.861111\n",
            "Validation loss decreased (0.480094 --> 0.478852).  Saving model ...\n",
            "Epoch: 25 \tTraining Loss: 0.420247 \tValidation Loss: 0.444097 \tAccuracy: 0.833333\n",
            "Validation loss decreased (0.478852 --> 0.444097).  Saving model ...\n",
            "Epoch: 26 \tTraining Loss: 0.444814 \tValidation Loss: 0.442472 \tAccuracy: 0.833333\n",
            "Validation loss decreased (0.444097 --> 0.442472).  Saving model ...\n",
            "Epoch: 27 \tTraining Loss: 0.375922 \tValidation Loss: 0.429731 \tAccuracy: 0.833333\n",
            "Validation loss decreased (0.442472 --> 0.429731).  Saving model ...\n",
            "Epoch: 28 \tTraining Loss: 0.362233 \tValidation Loss: 0.417754 \tAccuracy: 0.861111\n",
            "Validation loss decreased (0.429731 --> 0.417754).  Saving model ...\n",
            "Epoch: 29 \tTraining Loss: 0.349122 \tValidation Loss: 0.417306 \tAccuracy: 0.861111\n",
            "Validation loss decreased (0.417754 --> 0.417306).  Saving model ...\n",
            "Epoch: 30 \tTraining Loss: 0.324288 \tValidation Loss: 0.400360 \tAccuracy: 0.861111\n",
            "Validation loss decreased (0.417306 --> 0.400360).  Saving model ...\n",
            "Epoch: 31 \tTraining Loss: 0.369456 \tValidation Loss: 0.392602 \tAccuracy: 0.861111\n",
            "Validation loss decreased (0.400360 --> 0.392602).  Saving model ...\n",
            "Epoch: 32 \tTraining Loss: 0.367408 \tValidation Loss: 0.395162 \tAccuracy: 0.861111\n",
            "Epoch: 33 \tTraining Loss: 0.366645 \tValidation Loss: 0.380946 \tAccuracy: 0.861111\n",
            "Validation loss decreased (0.392602 --> 0.380946).  Saving model ...\n",
            "Epoch: 34 \tTraining Loss: 0.342187 \tValidation Loss: 0.375466 \tAccuracy: 0.861111\n",
            "Validation loss decreased (0.380946 --> 0.375466).  Saving model ...\n",
            "Epoch: 35 \tTraining Loss: 0.326860 \tValidation Loss: 0.384615 \tAccuracy: 0.861111\n",
            "Epoch: 36 \tTraining Loss: 0.347732 \tValidation Loss: 0.370148 \tAccuracy: 0.861111\n",
            "Validation loss decreased (0.375466 --> 0.370148).  Saving model ...\n",
            "Epoch: 37 \tTraining Loss: 0.321004 \tValidation Loss: 0.360784 \tAccuracy: 0.861111\n",
            "Validation loss decreased (0.370148 --> 0.360784).  Saving model ...\n",
            "Epoch: 38 \tTraining Loss: 0.344647 \tValidation Loss: 0.351127 \tAccuracy: 0.861111\n",
            "Validation loss decreased (0.360784 --> 0.351127).  Saving model ...\n",
            "Epoch: 39 \tTraining Loss: 0.288999 \tValidation Loss: 0.349536 \tAccuracy: 0.861111\n",
            "Validation loss decreased (0.351127 --> 0.349536).  Saving model ...\n",
            "Epoch: 40 \tTraining Loss: 0.291272 \tValidation Loss: 0.340519 \tAccuracy: 0.861111\n",
            "Validation loss decreased (0.349536 --> 0.340519).  Saving model ...\n",
            "Epoch: 41 \tTraining Loss: 0.285270 \tValidation Loss: 0.336184 \tAccuracy: 0.861111\n",
            "Validation loss decreased (0.340519 --> 0.336184).  Saving model ...\n",
            "Epoch: 42 \tTraining Loss: 0.315252 \tValidation Loss: 0.331789 \tAccuracy: 0.861111\n",
            "Validation loss decreased (0.336184 --> 0.331789).  Saving model ...\n",
            "Epoch: 43 \tTraining Loss: 0.287234 \tValidation Loss: 0.339835 \tAccuracy: 0.861111\n",
            "Epoch: 44 \tTraining Loss: 0.264377 \tValidation Loss: 0.320383 \tAccuracy: 0.861111\n",
            "Validation loss decreased (0.331789 --> 0.320383).  Saving model ...\n",
            "Epoch: 45 \tTraining Loss: 0.296261 \tValidation Loss: 0.320533 \tAccuracy: 0.861111\n",
            "Epoch: 46 \tTraining Loss: 0.292784 \tValidation Loss: 0.319018 \tAccuracy: 0.861111\n",
            "Validation loss decreased (0.320383 --> 0.319018).  Saving model ...\n",
            "Epoch: 47 \tTraining Loss: 0.283010 \tValidation Loss: 0.309266 \tAccuracy: 0.861111\n",
            "Validation loss decreased (0.319018 --> 0.309266).  Saving model ...\n",
            "Epoch: 48 \tTraining Loss: 0.261899 \tValidation Loss: 0.310674 \tAccuracy: 0.861111\n",
            "Epoch: 49 \tTraining Loss: 0.261784 \tValidation Loss: 0.310882 \tAccuracy: 0.888889\n",
            "Epoch: 50 \tTraining Loss: 0.247563 \tValidation Loss: 0.302488 \tAccuracy: 0.888889\n",
            "Validation loss decreased (0.309266 --> 0.302488).  Saving model ...\n",
            "Epoch: 51 \tTraining Loss: 0.242228 \tValidation Loss: 0.300027 \tAccuracy: 0.888889\n",
            "Validation loss decreased (0.302488 --> 0.300027).  Saving model ...\n",
            "Epoch: 52 \tTraining Loss: 0.271774 \tValidation Loss: 0.298913 \tAccuracy: 0.888889\n",
            "Validation loss decreased (0.300027 --> 0.298913).  Saving model ...\n",
            "Epoch: 53 \tTraining Loss: 0.236274 \tValidation Loss: 0.289774 \tAccuracy: 0.888889\n",
            "Validation loss decreased (0.298913 --> 0.289774).  Saving model ...\n",
            "Epoch: 54 \tTraining Loss: 0.259199 \tValidation Loss: 0.289965 \tAccuracy: 0.888889\n",
            "Epoch: 55 \tTraining Loss: 0.239310 \tValidation Loss: 0.293805 \tAccuracy: 0.888889\n",
            "Epoch: 56 \tTraining Loss: 0.203999 \tValidation Loss: 0.290537 \tAccuracy: 0.861111\n",
            "Epoch: 57 \tTraining Loss: 0.278528 \tValidation Loss: 0.286132 \tAccuracy: 0.888889\n",
            "Validation loss decreased (0.289774 --> 0.286132).  Saving model ...\n",
            "Epoch: 58 \tTraining Loss: 0.244507 \tValidation Loss: 0.286931 \tAccuracy: 0.861111\n",
            "Epoch: 59 \tTraining Loss: 0.261910 \tValidation Loss: 0.290636 \tAccuracy: 0.861111\n",
            "Epoch: 60 \tTraining Loss: 0.238432 \tValidation Loss: 0.281339 \tAccuracy: 0.888889\n",
            "Validation loss decreased (0.286132 --> 0.281339).  Saving model ...\n",
            "Epoch: 61 \tTraining Loss: 0.224597 \tValidation Loss: 0.276483 \tAccuracy: 0.888889\n",
            "Validation loss decreased (0.281339 --> 0.276483).  Saving model ...\n",
            "Epoch: 62 \tTraining Loss: 0.216471 \tValidation Loss: 0.280293 \tAccuracy: 0.888889\n",
            "Epoch: 63 \tTraining Loss: 0.244686 \tValidation Loss: 0.276528 \tAccuracy: 0.888889\n",
            "Epoch: 64 \tTraining Loss: 0.234198 \tValidation Loss: 0.269874 \tAccuracy: 0.888889\n",
            "Validation loss decreased (0.276483 --> 0.269874).  Saving model ...\n",
            "Epoch: 65 \tTraining Loss: 0.228031 \tValidation Loss: 0.269652 \tAccuracy: 0.888889\n",
            "Validation loss decreased (0.269874 --> 0.269652).  Saving model ...\n",
            "Epoch: 66 \tTraining Loss: 0.241733 \tValidation Loss: 0.266523 \tAccuracy: 0.888889\n",
            "Validation loss decreased (0.269652 --> 0.266523).  Saving model ...\n",
            "Epoch: 67 \tTraining Loss: 0.201895 \tValidation Loss: 0.267755 \tAccuracy: 0.888889\n",
            "Epoch: 68 \tTraining Loss: 0.206130 \tValidation Loss: 0.274317 \tAccuracy: 0.888889\n",
            "Epoch: 69 \tTraining Loss: 0.192461 \tValidation Loss: 0.271904 \tAccuracy: 0.888889\n",
            "Epoch: 70 \tTraining Loss: 0.247135 \tValidation Loss: 0.270585 \tAccuracy: 0.888889\n",
            "Epoch: 71 \tTraining Loss: 0.232988 \tValidation Loss: 0.268286 \tAccuracy: 0.888889\n",
            "Epoch: 72 \tTraining Loss: 0.187909 \tValidation Loss: 0.261458 \tAccuracy: 0.888889\n",
            "Validation loss decreased (0.266523 --> 0.261458).  Saving model ...\n",
            "Epoch: 73 \tTraining Loss: 0.180493 \tValidation Loss: 0.252875 \tAccuracy: 0.888889\n",
            "Validation loss decreased (0.261458 --> 0.252875).  Saving model ...\n",
            "Epoch: 74 \tTraining Loss: 0.202042 \tValidation Loss: 0.251038 \tAccuracy: 0.888889\n",
            "Validation loss decreased (0.252875 --> 0.251038).  Saving model ...\n",
            "Epoch: 75 \tTraining Loss: 0.223502 \tValidation Loss: 0.251143 \tAccuracy: 0.888889\n",
            "Epoch: 76 \tTraining Loss: 0.192195 \tValidation Loss: 0.252538 \tAccuracy: 0.888889\n",
            "Epoch: 77 \tTraining Loss: 0.213523 \tValidation Loss: 0.249968 \tAccuracy: 0.888889\n",
            "Validation loss decreased (0.251038 --> 0.249968).  Saving model ...\n",
            "Epoch: 78 \tTraining Loss: 0.220266 \tValidation Loss: 0.246896 \tAccuracy: 0.888889\n",
            "Validation loss decreased (0.249968 --> 0.246896).  Saving model ...\n",
            "Epoch: 79 \tTraining Loss: 0.238726 \tValidation Loss: 0.241091 \tAccuracy: 0.888889\n",
            "Validation loss decreased (0.246896 --> 0.241091).  Saving model ...\n",
            "Epoch: 80 \tTraining Loss: 0.211173 \tValidation Loss: 0.243287 \tAccuracy: 0.888889\n",
            "Epoch: 81 \tTraining Loss: 0.230725 \tValidation Loss: 0.244021 \tAccuracy: 0.888889\n",
            "Epoch: 82 \tTraining Loss: 0.225913 \tValidation Loss: 0.241414 \tAccuracy: 0.888889\n",
            "Epoch: 83 \tTraining Loss: 0.179000 \tValidation Loss: 0.244022 \tAccuracy: 0.888889\n",
            "Epoch: 84 \tTraining Loss: 0.211833 \tValidation Loss: 0.246573 \tAccuracy: 0.888889\n",
            "Epoch: 85 \tTraining Loss: 0.216232 \tValidation Loss: 0.251376 \tAccuracy: 0.888889\n",
            "Epoch: 86 \tTraining Loss: 0.219595 \tValidation Loss: 0.254789 \tAccuracy: 0.888889\n",
            "Epoch: 87 \tTraining Loss: 0.187709 \tValidation Loss: 0.247140 \tAccuracy: 0.888889\n",
            "Epoch: 88 \tTraining Loss: 0.168693 \tValidation Loss: 0.247539 \tAccuracy: 0.888889\n",
            "Epoch: 89 \tTraining Loss: 0.188288 \tValidation Loss: 0.246685 \tAccuracy: 0.888889\n",
            "Epoch: 90 \tTraining Loss: 0.186669 \tValidation Loss: 0.245870 \tAccuracy: 0.888889\n",
            "Epoch: 91 \tTraining Loss: 0.192871 \tValidation Loss: 0.242910 \tAccuracy: 0.888889\n",
            "Epoch: 92 \tTraining Loss: 0.195425 \tValidation Loss: 0.248502 \tAccuracy: 0.888889\n",
            "Epoch: 93 \tTraining Loss: 0.180961 \tValidation Loss: 0.246281 \tAccuracy: 0.888889\n",
            "Epoch: 94 \tTraining Loss: 0.212184 \tValidation Loss: 0.241039 \tAccuracy: 0.888889\n",
            "Validation loss decreased (0.241091 --> 0.241039).  Saving model ...\n",
            "Epoch: 95 \tTraining Loss: 0.147534 \tValidation Loss: 0.239327 \tAccuracy: 0.888889\n",
            "Validation loss decreased (0.241039 --> 0.239327).  Saving model ...\n",
            "Epoch: 96 \tTraining Loss: 0.183507 \tValidation Loss: 0.241936 \tAccuracy: 0.888889\n",
            "Epoch: 97 \tTraining Loss: 0.179674 \tValidation Loss: 0.239606 \tAccuracy: 0.888889\n",
            "Epoch: 98 \tTraining Loss: 0.178978 \tValidation Loss: 0.238664 \tAccuracy: 0.888889\n",
            "Validation loss decreased (0.239327 --> 0.238664).  Saving model ...\n",
            "Epoch: 99 \tTraining Loss: 0.206045 \tValidation Loss: 0.242334 \tAccuracy: 0.888889\n",
            "Epoch: 100 \tTraining Loss: 0.193180 \tValidation Loss: 0.233752 \tAccuracy: 0.888889\n",
            "Validation loss decreased (0.238664 --> 0.233752).  Saving model ...\n",
            "Epoch: 101 \tTraining Loss: 0.161759 \tValidation Loss: 0.230314 \tAccuracy: 0.888889\n",
            "Validation loss decreased (0.233752 --> 0.230314).  Saving model ...\n",
            "Epoch: 102 \tTraining Loss: 0.210456 \tValidation Loss: 0.227840 \tAccuracy: 0.888889\n",
            "Validation loss decreased (0.230314 --> 0.227840).  Saving model ...\n",
            "Epoch: 103 \tTraining Loss: 0.160537 \tValidation Loss: 0.225003 \tAccuracy: 0.888889\n",
            "Validation loss decreased (0.227840 --> 0.225003).  Saving model ...\n",
            "Epoch: 104 \tTraining Loss: 0.162369 \tValidation Loss: 0.230817 \tAccuracy: 0.888889\n",
            "Epoch: 105 \tTraining Loss: 0.183948 \tValidation Loss: 0.231435 \tAccuracy: 0.916667\n",
            "Epoch: 106 \tTraining Loss: 0.160688 \tValidation Loss: 0.234685 \tAccuracy: 0.888889\n",
            "Epoch: 107 \tTraining Loss: 0.173101 \tValidation Loss: 0.242615 \tAccuracy: 0.888889\n",
            "Epoch: 108 \tTraining Loss: 0.179820 \tValidation Loss: 0.245545 \tAccuracy: 0.888889\n",
            "Epoch: 109 \tTraining Loss: 0.203264 \tValidation Loss: 0.237783 \tAccuracy: 0.916667\n",
            "Epoch: 110 \tTraining Loss: 0.167009 \tValidation Loss: 0.235602 \tAccuracy: 0.916667\n",
            "Epoch: 111 \tTraining Loss: 0.168368 \tValidation Loss: 0.235072 \tAccuracy: 0.916667\n",
            "Epoch: 112 \tTraining Loss: 0.197638 \tValidation Loss: 0.234427 \tAccuracy: 0.916667\n",
            "Epoch: 113 \tTraining Loss: 0.210331 \tValidation Loss: 0.235001 \tAccuracy: 0.888889\n",
            "Epoch: 114 \tTraining Loss: 0.186128 \tValidation Loss: 0.232007 \tAccuracy: 0.888889\n",
            "Epoch: 115 \tTraining Loss: 0.158746 \tValidation Loss: 0.229653 \tAccuracy: 0.888889\n",
            "Epoch: 116 \tTraining Loss: 0.160648 \tValidation Loss: 0.226497 \tAccuracy: 0.916667\n",
            "Epoch: 117 \tTraining Loss: 0.167532 \tValidation Loss: 0.227614 \tAccuracy: 0.888889\n",
            "Epoch: 118 \tTraining Loss: 0.163651 \tValidation Loss: 0.226267 \tAccuracy: 0.916667\n",
            "Epoch: 119 \tTraining Loss: 0.167739 \tValidation Loss: 0.225788 \tAccuracy: 0.888889\n",
            "Epoch: 120 \tTraining Loss: 0.185808 \tValidation Loss: 0.231723 \tAccuracy: 0.888889\n",
            "Epoch: 121 \tTraining Loss: 0.164564 \tValidation Loss: 0.234480 \tAccuracy: 0.888889\n",
            "Epoch: 122 \tTraining Loss: 0.180219 \tValidation Loss: 0.232625 \tAccuracy: 0.888889\n",
            "Epoch: 123 \tTraining Loss: 0.156108 \tValidation Loss: 0.227651 \tAccuracy: 0.888889\n",
            "Epoch: 124 \tTraining Loss: 0.169166 \tValidation Loss: 0.227074 \tAccuracy: 0.888889\n",
            "Epoch: 125 \tTraining Loss: 0.137261 \tValidation Loss: 0.224962 \tAccuracy: 0.888889\n",
            "Validation loss decreased (0.225003 --> 0.224962).  Saving model ...\n",
            "Epoch: 126 \tTraining Loss: 0.184909 \tValidation Loss: 0.224309 \tAccuracy: 0.888889\n",
            "Validation loss decreased (0.224962 --> 0.224309).  Saving model ...\n",
            "Epoch: 127 \tTraining Loss: 0.176988 \tValidation Loss: 0.226165 \tAccuracy: 0.888889\n",
            "Epoch: 128 \tTraining Loss: 0.140225 \tValidation Loss: 0.224446 \tAccuracy: 0.888889\n",
            "Epoch: 129 \tTraining Loss: 0.159512 \tValidation Loss: 0.226412 \tAccuracy: 0.888889\n",
            "Epoch: 130 \tTraining Loss: 0.144512 \tValidation Loss: 0.228951 \tAccuracy: 0.888889\n",
            "Epoch: 131 \tTraining Loss: 0.192336 \tValidation Loss: 0.236109 \tAccuracy: 0.888889\n",
            "Epoch: 132 \tTraining Loss: 0.132889 \tValidation Loss: 0.232245 \tAccuracy: 0.888889\n",
            "Epoch: 133 \tTraining Loss: 0.158762 \tValidation Loss: 0.228644 \tAccuracy: 0.888889\n",
            "Epoch: 134 \tTraining Loss: 0.154614 \tValidation Loss: 0.230200 \tAccuracy: 0.888889\n",
            "Epoch: 135 \tTraining Loss: 0.125063 \tValidation Loss: 0.229129 \tAccuracy: 0.916667\n",
            "Epoch: 136 \tTraining Loss: 0.129490 \tValidation Loss: 0.228640 \tAccuracy: 0.888889\n",
            "Epoch: 137 \tTraining Loss: 0.147869 \tValidation Loss: 0.231366 \tAccuracy: 0.888889\n",
            "Epoch: 138 \tTraining Loss: 0.139002 \tValidation Loss: 0.228070 \tAccuracy: 0.888889\n",
            "Epoch: 139 \tTraining Loss: 0.138838 \tValidation Loss: 0.231821 \tAccuracy: 0.888889\n",
            "Epoch: 140 \tTraining Loss: 0.164792 \tValidation Loss: 0.230586 \tAccuracy: 0.888889\n",
            "Epoch: 141 \tTraining Loss: 0.118906 \tValidation Loss: 0.229635 \tAccuracy: 0.888889\n",
            "Epoch: 142 \tTraining Loss: 0.136473 \tValidation Loss: 0.225613 \tAccuracy: 0.888889\n",
            "Epoch: 143 \tTraining Loss: 0.133218 \tValidation Loss: 0.226890 \tAccuracy: 0.888889\n",
            "Epoch: 144 \tTraining Loss: 0.133273 \tValidation Loss: 0.225761 \tAccuracy: 0.888889\n",
            "Epoch: 145 \tTraining Loss: 0.197476 \tValidation Loss: 0.224150 \tAccuracy: 0.888889\n",
            "Validation loss decreased (0.224309 --> 0.224150).  Saving model ...\n",
            "Epoch: 146 \tTraining Loss: 0.136067 \tValidation Loss: 0.227015 \tAccuracy: 0.888889\n",
            "Epoch: 147 \tTraining Loss: 0.139566 \tValidation Loss: 0.221727 \tAccuracy: 0.888889\n",
            "Validation loss decreased (0.224150 --> 0.221727).  Saving model ...\n",
            "Epoch: 148 \tTraining Loss: 0.158799 \tValidation Loss: 0.224199 \tAccuracy: 0.888889\n",
            "Epoch: 149 \tTraining Loss: 0.153335 \tValidation Loss: 0.222543 \tAccuracy: 0.888889\n",
            "Epoch: 150 \tTraining Loss: 0.179126 \tValidation Loss: 0.231058 \tAccuracy: 0.888889\n",
            "Epoch: 151 \tTraining Loss: 0.157139 \tValidation Loss: 0.231324 \tAccuracy: 0.888889\n",
            "Epoch: 152 \tTraining Loss: 0.139898 \tValidation Loss: 0.229077 \tAccuracy: 0.916667\n",
            "Epoch: 153 \tTraining Loss: 0.132954 \tValidation Loss: 0.226808 \tAccuracy: 0.916667\n",
            "Epoch: 154 \tTraining Loss: 0.162061 \tValidation Loss: 0.229759 \tAccuracy: 0.916667\n",
            "Epoch: 155 \tTraining Loss: 0.172793 \tValidation Loss: 0.228366 \tAccuracy: 0.916667\n",
            "Epoch: 156 \tTraining Loss: 0.133668 \tValidation Loss: 0.225916 \tAccuracy: 0.916667\n",
            "Epoch: 157 \tTraining Loss: 0.123805 \tValidation Loss: 0.223925 \tAccuracy: 0.916667\n",
            "Epoch: 158 \tTraining Loss: 0.168867 \tValidation Loss: 0.220303 \tAccuracy: 0.916667\n",
            "Validation loss decreased (0.221727 --> 0.220303).  Saving model ...\n",
            "Epoch: 159 \tTraining Loss: 0.150711 \tValidation Loss: 0.219250 \tAccuracy: 0.916667\n",
            "Validation loss decreased (0.220303 --> 0.219250).  Saving model ...\n",
            "Epoch: 160 \tTraining Loss: 0.162869 \tValidation Loss: 0.218169 \tAccuracy: 0.916667\n",
            "Validation loss decreased (0.219250 --> 0.218169).  Saving model ...\n",
            "Epoch: 161 \tTraining Loss: 0.152136 \tValidation Loss: 0.220946 \tAccuracy: 0.916667\n",
            "Epoch: 162 \tTraining Loss: 0.133409 \tValidation Loss: 0.220286 \tAccuracy: 0.916667\n",
            "Epoch: 163 \tTraining Loss: 0.132702 \tValidation Loss: 0.220336 \tAccuracy: 0.916667\n",
            "Epoch: 164 \tTraining Loss: 0.176150 \tValidation Loss: 0.220855 \tAccuracy: 0.916667\n",
            "Epoch: 165 \tTraining Loss: 0.123596 \tValidation Loss: 0.222297 \tAccuracy: 0.916667\n",
            "Epoch: 166 \tTraining Loss: 0.138564 \tValidation Loss: 0.216201 \tAccuracy: 0.916667\n",
            "Validation loss decreased (0.218169 --> 0.216201).  Saving model ...\n",
            "Epoch: 167 \tTraining Loss: 0.169189 \tValidation Loss: 0.215020 \tAccuracy: 0.916667\n",
            "Validation loss decreased (0.216201 --> 0.215020).  Saving model ...\n",
            "Epoch: 168 \tTraining Loss: 0.141453 \tValidation Loss: 0.213309 \tAccuracy: 0.916667\n",
            "Validation loss decreased (0.215020 --> 0.213309).  Saving model ...\n",
            "Epoch: 169 \tTraining Loss: 0.138102 \tValidation Loss: 0.212390 \tAccuracy: 0.916667\n",
            "Validation loss decreased (0.213309 --> 0.212390).  Saving model ...\n",
            "Epoch: 170 \tTraining Loss: 0.133753 \tValidation Loss: 0.215914 \tAccuracy: 0.916667\n",
            "Epoch: 171 \tTraining Loss: 0.105305 \tValidation Loss: 0.214065 \tAccuracy: 0.916667\n",
            "Epoch: 172 \tTraining Loss: 0.119121 \tValidation Loss: 0.214635 \tAccuracy: 0.916667\n",
            "Epoch: 173 \tTraining Loss: 0.107224 \tValidation Loss: 0.217235 \tAccuracy: 0.916667\n",
            "Epoch: 174 \tTraining Loss: 0.149559 \tValidation Loss: 0.218507 \tAccuracy: 0.888889\n",
            "Epoch: 175 \tTraining Loss: 0.130737 \tValidation Loss: 0.216487 \tAccuracy: 0.916667\n",
            "Epoch: 176 \tTraining Loss: 0.116468 \tValidation Loss: 0.216664 \tAccuracy: 0.888889\n",
            "Epoch: 177 \tTraining Loss: 0.119785 \tValidation Loss: 0.217739 \tAccuracy: 0.916667\n",
            "Epoch: 178 \tTraining Loss: 0.138700 \tValidation Loss: 0.217953 \tAccuracy: 0.916667\n",
            "Epoch: 179 \tTraining Loss: 0.166628 \tValidation Loss: 0.214540 \tAccuracy: 0.916667\n",
            "Epoch: 180 \tTraining Loss: 0.142486 \tValidation Loss: 0.211835 \tAccuracy: 0.916667\n",
            "Validation loss decreased (0.212390 --> 0.211835).  Saving model ...\n",
            "Epoch: 181 \tTraining Loss: 0.129066 \tValidation Loss: 0.211223 \tAccuracy: 0.916667\n",
            "Validation loss decreased (0.211835 --> 0.211223).  Saving model ...\n",
            "Epoch: 182 \tTraining Loss: 0.127656 \tValidation Loss: 0.211193 \tAccuracy: 0.916667\n",
            "Validation loss decreased (0.211223 --> 0.211193).  Saving model ...\n",
            "Epoch: 183 \tTraining Loss: 0.148513 \tValidation Loss: 0.209217 \tAccuracy: 0.916667\n",
            "Validation loss decreased (0.211193 --> 0.209217).  Saving model ...\n",
            "Epoch: 184 \tTraining Loss: 0.095532 \tValidation Loss: 0.210757 \tAccuracy: 0.916667\n",
            "Epoch: 185 \tTraining Loss: 0.118529 \tValidation Loss: 0.211651 \tAccuracy: 0.916667\n",
            "Epoch: 186 \tTraining Loss: 0.133403 \tValidation Loss: 0.212503 \tAccuracy: 0.916667\n",
            "Epoch: 187 \tTraining Loss: 0.137722 \tValidation Loss: 0.209524 \tAccuracy: 0.916667\n",
            "Epoch: 188 \tTraining Loss: 0.114481 \tValidation Loss: 0.210980 \tAccuracy: 0.916667\n",
            "Epoch: 189 \tTraining Loss: 0.184702 \tValidation Loss: 0.211868 \tAccuracy: 0.916667\n",
            "Epoch: 190 \tTraining Loss: 0.124424 \tValidation Loss: 0.210799 \tAccuracy: 0.916667\n",
            "Epoch: 191 \tTraining Loss: 0.139899 \tValidation Loss: 0.211489 \tAccuracy: 0.916667\n",
            "Epoch: 192 \tTraining Loss: 0.128144 \tValidation Loss: 0.208659 \tAccuracy: 0.916667\n",
            "Validation loss decreased (0.209217 --> 0.208659).  Saving model ...\n",
            "Epoch: 193 \tTraining Loss: 0.126890 \tValidation Loss: 0.210418 \tAccuracy: 0.916667\n",
            "Epoch: 194 \tTraining Loss: 0.122335 \tValidation Loss: 0.211354 \tAccuracy: 0.916667\n",
            "Epoch: 195 \tTraining Loss: 0.142003 \tValidation Loss: 0.210624 \tAccuracy: 0.916667\n",
            "Epoch: 196 \tTraining Loss: 0.145967 \tValidation Loss: 0.214078 \tAccuracy: 0.916667\n",
            "Epoch: 197 \tTraining Loss: 0.143010 \tValidation Loss: 0.217534 \tAccuracy: 0.888889\n",
            "Epoch: 198 \tTraining Loss: 0.161156 \tValidation Loss: 0.214903 \tAccuracy: 0.888889\n",
            "Epoch: 199 \tTraining Loss: 0.119044 \tValidation Loss: 0.214466 \tAccuracy: 0.916667\n",
            "Epoch: 200 \tTraining Loss: 0.107912 \tValidation Loss: 0.212618 \tAccuracy: 0.916667\n",
            "Epoch: 201 \tTraining Loss: 0.116113 \tValidation Loss: 0.211293 \tAccuracy: 0.916667\n",
            "Epoch: 202 \tTraining Loss: 0.145458 \tValidation Loss: 0.211398 \tAccuracy: 0.916667\n",
            "Epoch: 203 \tTraining Loss: 0.168942 \tValidation Loss: 0.213349 \tAccuracy: 0.916667\n",
            "Epoch: 204 \tTraining Loss: 0.127945 \tValidation Loss: 0.212628 \tAccuracy: 0.916667\n",
            "Epoch: 205 \tTraining Loss: 0.138605 \tValidation Loss: 0.213861 \tAccuracy: 0.916667\n",
            "Epoch: 206 \tTraining Loss: 0.116664 \tValidation Loss: 0.212936 \tAccuracy: 0.916667\n",
            "Epoch: 207 \tTraining Loss: 0.107963 \tValidation Loss: 0.211538 \tAccuracy: 0.916667\n",
            "Epoch: 208 \tTraining Loss: 0.123382 \tValidation Loss: 0.212058 \tAccuracy: 0.916667\n",
            "Epoch: 209 \tTraining Loss: 0.123925 \tValidation Loss: 0.211709 \tAccuracy: 0.916667\n",
            "Epoch: 210 \tTraining Loss: 0.118096 \tValidation Loss: 0.210428 \tAccuracy: 0.916667\n",
            "Epoch: 211 \tTraining Loss: 0.129779 \tValidation Loss: 0.210143 \tAccuracy: 0.916667\n",
            "Epoch: 212 \tTraining Loss: 0.141924 \tValidation Loss: 0.210486 \tAccuracy: 0.916667\n",
            "Epoch: 213 \tTraining Loss: 0.110022 \tValidation Loss: 0.210778 \tAccuracy: 0.916667\n",
            "Epoch: 214 \tTraining Loss: 0.169121 \tValidation Loss: 0.212134 \tAccuracy: 0.916667\n",
            "Epoch: 215 \tTraining Loss: 0.130442 \tValidation Loss: 0.210162 \tAccuracy: 0.916667\n",
            "Epoch: 216 \tTraining Loss: 0.126975 \tValidation Loss: 0.207333 \tAccuracy: 0.916667\n",
            "Validation loss decreased (0.208659 --> 0.207333).  Saving model ...\n",
            "Epoch: 217 \tTraining Loss: 0.124543 \tValidation Loss: 0.206122 \tAccuracy: 0.916667\n",
            "Validation loss decreased (0.207333 --> 0.206122).  Saving model ...\n",
            "Epoch: 218 \tTraining Loss: 0.146943 \tValidation Loss: 0.207916 \tAccuracy: 0.916667\n",
            "Epoch: 219 \tTraining Loss: 0.104831 \tValidation Loss: 0.209678 \tAccuracy: 0.916667\n",
            "Epoch: 220 \tTraining Loss: 0.098023 \tValidation Loss: 0.210013 \tAccuracy: 0.916667\n",
            "Epoch: 221 \tTraining Loss: 0.148876 \tValidation Loss: 0.209416 \tAccuracy: 0.916667\n",
            "Epoch: 222 \tTraining Loss: 0.108467 \tValidation Loss: 0.207356 \tAccuracy: 0.916667\n",
            "Epoch: 223 \tTraining Loss: 0.120451 \tValidation Loss: 0.208501 \tAccuracy: 0.916667\n",
            "Epoch: 224 \tTraining Loss: 0.145146 \tValidation Loss: 0.209455 \tAccuracy: 0.916667\n",
            "Epoch: 225 \tTraining Loss: 0.101120 \tValidation Loss: 0.211249 \tAccuracy: 0.916667\n",
            "Epoch: 226 \tTraining Loss: 0.145816 \tValidation Loss: 0.211240 \tAccuracy: 0.916667\n",
            "Epoch: 227 \tTraining Loss: 0.115291 \tValidation Loss: 0.211962 \tAccuracy: 0.916667\n",
            "Epoch: 228 \tTraining Loss: 0.104540 \tValidation Loss: 0.212103 \tAccuracy: 0.916667\n",
            "Epoch: 229 \tTraining Loss: 0.093372 \tValidation Loss: 0.212786 \tAccuracy: 0.916667\n",
            "Epoch: 230 \tTraining Loss: 0.113066 \tValidation Loss: 0.211692 \tAccuracy: 0.916667\n",
            "Epoch: 231 \tTraining Loss: 0.106574 \tValidation Loss: 0.212511 \tAccuracy: 0.916667\n",
            "Epoch: 232 \tTraining Loss: 0.117713 \tValidation Loss: 0.212696 \tAccuracy: 0.916667\n",
            "Epoch: 233 \tTraining Loss: 0.134624 \tValidation Loss: 0.212952 \tAccuracy: 0.916667\n",
            "Epoch: 234 \tTraining Loss: 0.125491 \tValidation Loss: 0.212707 \tAccuracy: 0.916667\n",
            "Epoch: 235 \tTraining Loss: 0.145482 \tValidation Loss: 0.213634 \tAccuracy: 0.916667\n",
            "Epoch: 236 \tTraining Loss: 0.129745 \tValidation Loss: 0.212805 \tAccuracy: 0.916667\n",
            "Epoch: 237 \tTraining Loss: 0.141221 \tValidation Loss: 0.214741 \tAccuracy: 0.916667\n",
            "Epoch: 238 \tTraining Loss: 0.135034 \tValidation Loss: 0.215012 \tAccuracy: 0.916667\n",
            "Epoch: 239 \tTraining Loss: 0.133231 \tValidation Loss: 0.215897 \tAccuracy: 0.916667\n",
            "Epoch: 240 \tTraining Loss: 0.151006 \tValidation Loss: 0.217979 \tAccuracy: 0.916667\n",
            "Epoch: 241 \tTraining Loss: 0.112575 \tValidation Loss: 0.214895 \tAccuracy: 0.916667\n",
            "Epoch: 242 \tTraining Loss: 0.172043 \tValidation Loss: 0.214657 \tAccuracy: 0.916667\n",
            "Epoch: 243 \tTraining Loss: 0.108437 \tValidation Loss: 0.212884 \tAccuracy: 0.916667\n",
            "Epoch: 244 \tTraining Loss: 0.114557 \tValidation Loss: 0.214727 \tAccuracy: 0.916667\n",
            "Epoch: 245 \tTraining Loss: 0.133071 \tValidation Loss: 0.214688 \tAccuracy: 0.916667\n",
            "Epoch: 246 \tTraining Loss: 0.125374 \tValidation Loss: 0.214419 \tAccuracy: 0.916667\n",
            "Epoch: 247 \tTraining Loss: 0.129826 \tValidation Loss: 0.215057 \tAccuracy: 0.916667\n",
            "Epoch: 248 \tTraining Loss: 0.108333 \tValidation Loss: 0.214421 \tAccuracy: 0.916667\n",
            "Epoch: 249 \tTraining Loss: 0.141843 \tValidation Loss: 0.211806 \tAccuracy: 0.916667\n",
            "Epoch: 250 \tTraining Loss: 0.118821 \tValidation Loss: 0.210340 \tAccuracy: 0.916667\n",
            "Epoch: 251 \tTraining Loss: 0.130405 \tValidation Loss: 0.210939 \tAccuracy: 0.916667\n",
            "Epoch: 252 \tTraining Loss: 0.125956 \tValidation Loss: 0.211383 \tAccuracy: 0.916667\n",
            "Epoch: 253 \tTraining Loss: 0.118922 \tValidation Loss: 0.211609 \tAccuracy: 0.916667\n",
            "Epoch: 254 \tTraining Loss: 0.122501 \tValidation Loss: 0.210961 \tAccuracy: 0.916667\n",
            "Epoch: 255 \tTraining Loss: 0.116078 \tValidation Loss: 0.209835 \tAccuracy: 0.916667\n",
            "Epoch: 256 \tTraining Loss: 0.121157 \tValidation Loss: 0.210659 \tAccuracy: 0.916667\n",
            "Epoch: 257 \tTraining Loss: 0.115942 \tValidation Loss: 0.210882 \tAccuracy: 0.916667\n",
            "Epoch: 258 \tTraining Loss: 0.141003 \tValidation Loss: 0.207907 \tAccuracy: 0.916667\n",
            "Epoch: 259 \tTraining Loss: 0.106088 \tValidation Loss: 0.205960 \tAccuracy: 0.916667\n",
            "Validation loss decreased (0.206122 --> 0.205960).  Saving model ...\n",
            "Epoch: 260 \tTraining Loss: 0.124680 \tValidation Loss: 0.206067 \tAccuracy: 0.916667\n",
            "Epoch: 261 \tTraining Loss: 0.143664 \tValidation Loss: 0.204880 \tAccuracy: 0.916667\n",
            "Validation loss decreased (0.205960 --> 0.204880).  Saving model ...\n",
            "Epoch: 262 \tTraining Loss: 0.081749 \tValidation Loss: 0.204459 \tAccuracy: 0.916667\n",
            "Validation loss decreased (0.204880 --> 0.204459).  Saving model ...\n",
            "Epoch: 263 \tTraining Loss: 0.140062 \tValidation Loss: 0.204320 \tAccuracy: 0.916667\n",
            "Validation loss decreased (0.204459 --> 0.204320).  Saving model ...\n",
            "Epoch: 264 \tTraining Loss: 0.124818 \tValidation Loss: 0.203838 \tAccuracy: 0.916667\n",
            "Validation loss decreased (0.204320 --> 0.203838).  Saving model ...\n",
            "Epoch: 265 \tTraining Loss: 0.115287 \tValidation Loss: 0.202724 \tAccuracy: 0.916667\n",
            "Validation loss decreased (0.203838 --> 0.202724).  Saving model ...\n",
            "Epoch: 266 \tTraining Loss: 0.085141 \tValidation Loss: 0.203086 \tAccuracy: 0.916667\n",
            "Epoch: 267 \tTraining Loss: 0.105986 \tValidation Loss: 0.203966 \tAccuracy: 0.916667\n",
            "Epoch: 268 \tTraining Loss: 0.147952 \tValidation Loss: 0.207037 \tAccuracy: 0.916667\n",
            "Epoch: 269 \tTraining Loss: 0.104432 \tValidation Loss: 0.208420 \tAccuracy: 0.916667\n",
            "Epoch: 270 \tTraining Loss: 0.112170 \tValidation Loss: 0.209023 \tAccuracy: 0.916667\n",
            "Epoch: 271 \tTraining Loss: 0.137621 \tValidation Loss: 0.209329 \tAccuracy: 0.916667\n",
            "Epoch: 272 \tTraining Loss: 0.153329 \tValidation Loss: 0.207746 \tAccuracy: 0.916667\n",
            "Epoch: 273 \tTraining Loss: 0.121635 \tValidation Loss: 0.206601 \tAccuracy: 0.916667\n",
            "Epoch: 274 \tTraining Loss: 0.104702 \tValidation Loss: 0.206868 \tAccuracy: 0.916667\n",
            "Epoch: 275 \tTraining Loss: 0.107740 \tValidation Loss: 0.207010 \tAccuracy: 0.916667\n",
            "Epoch: 276 \tTraining Loss: 0.121467 \tValidation Loss: 0.205720 \tAccuracy: 0.916667\n",
            "Epoch: 277 \tTraining Loss: 0.084914 \tValidation Loss: 0.206666 \tAccuracy: 0.916667\n",
            "Epoch: 278 \tTraining Loss: 0.130951 \tValidation Loss: 0.206563 \tAccuracy: 0.916667\n",
            "Epoch: 279 \tTraining Loss: 0.111915 \tValidation Loss: 0.204786 \tAccuracy: 0.916667\n",
            "Epoch: 280 \tTraining Loss: 0.100392 \tValidation Loss: 0.205631 \tAccuracy: 0.916667\n",
            "Epoch: 281 \tTraining Loss: 0.105885 \tValidation Loss: 0.204736 \tAccuracy: 0.916667\n",
            "Epoch: 282 \tTraining Loss: 0.084465 \tValidation Loss: 0.204337 \tAccuracy: 0.916667\n",
            "Epoch: 283 \tTraining Loss: 0.107235 \tValidation Loss: 0.204775 \tAccuracy: 0.916667\n",
            "Epoch: 284 \tTraining Loss: 0.124911 \tValidation Loss: 0.205001 \tAccuracy: 0.916667\n",
            "Epoch: 285 \tTraining Loss: 0.103250 \tValidation Loss: 0.205605 \tAccuracy: 0.916667\n",
            "Epoch: 286 \tTraining Loss: 0.088475 \tValidation Loss: 0.206352 \tAccuracy: 0.916667\n",
            "Epoch: 287 \tTraining Loss: 0.120284 \tValidation Loss: 0.205111 \tAccuracy: 0.916667\n",
            "Epoch: 288 \tTraining Loss: 0.115517 \tValidation Loss: 0.206366 \tAccuracy: 0.916667\n",
            "Epoch: 289 \tTraining Loss: 0.132625 \tValidation Loss: 0.204535 \tAccuracy: 0.916667\n",
            "Epoch: 290 \tTraining Loss: 0.103317 \tValidation Loss: 0.205049 \tAccuracy: 0.916667\n",
            "Epoch: 291 \tTraining Loss: 0.098240 \tValidation Loss: 0.204398 \tAccuracy: 0.916667\n",
            "Epoch: 292 \tTraining Loss: 0.110342 \tValidation Loss: 0.205219 \tAccuracy: 0.916667\n",
            "Epoch: 293 \tTraining Loss: 0.116628 \tValidation Loss: 0.207358 \tAccuracy: 0.916667\n",
            "Epoch: 294 \tTraining Loss: 0.121294 \tValidation Loss: 0.207980 \tAccuracy: 0.916667\n",
            "Epoch: 295 \tTraining Loss: 0.094518 \tValidation Loss: 0.207479 \tAccuracy: 0.916667\n",
            "Epoch: 296 \tTraining Loss: 0.086480 \tValidation Loss: 0.206997 \tAccuracy: 0.916667\n",
            "Epoch: 297 \tTraining Loss: 0.120966 \tValidation Loss: 0.207162 \tAccuracy: 0.916667\n",
            "Epoch: 298 \tTraining Loss: 0.092504 \tValidation Loss: 0.207822 \tAccuracy: 0.916667\n",
            "Epoch: 299 \tTraining Loss: 0.097870 \tValidation Loss: 0.206926 \tAccuracy: 0.916667\n",
            "Epoch: 300 \tTraining Loss: 0.113726 \tValidation Loss: 0.206133 \tAccuracy: 0.916667\n",
            "Epoch: 301 \tTraining Loss: 0.123177 \tValidation Loss: 0.208238 \tAccuracy: 0.916667\n",
            "Epoch: 302 \tTraining Loss: 0.125417 \tValidation Loss: 0.208463 \tAccuracy: 0.916667\n",
            "Epoch: 303 \tTraining Loss: 0.116851 \tValidation Loss: 0.208058 \tAccuracy: 0.916667\n",
            "Epoch: 304 \tTraining Loss: 0.099820 \tValidation Loss: 0.208830 \tAccuracy: 0.916667\n",
            "Epoch: 305 \tTraining Loss: 0.094139 \tValidation Loss: 0.208272 \tAccuracy: 0.916667\n",
            "Epoch: 306 \tTraining Loss: 0.108589 \tValidation Loss: 0.207313 \tAccuracy: 0.916667\n",
            "Epoch: 307 \tTraining Loss: 0.099224 \tValidation Loss: 0.205882 \tAccuracy: 0.916667\n",
            "Epoch: 308 \tTraining Loss: 0.111437 \tValidation Loss: 0.205358 \tAccuracy: 0.916667\n",
            "Epoch: 309 \tTraining Loss: 0.084372 \tValidation Loss: 0.205292 \tAccuracy: 0.916667\n",
            "Epoch: 310 \tTraining Loss: 0.106779 \tValidation Loss: 0.207309 \tAccuracy: 0.916667\n",
            "Epoch: 311 \tTraining Loss: 0.130873 \tValidation Loss: 0.206215 \tAccuracy: 0.916667\n",
            "Epoch: 312 \tTraining Loss: 0.126145 \tValidation Loss: 0.205407 \tAccuracy: 0.916667\n",
            "Epoch: 313 \tTraining Loss: 0.155501 \tValidation Loss: 0.204965 \tAccuracy: 0.916667\n",
            "Epoch: 314 \tTraining Loss: 0.108958 \tValidation Loss: 0.204934 \tAccuracy: 0.916667\n",
            "Epoch: 315 \tTraining Loss: 0.117925 \tValidation Loss: 0.202701 \tAccuracy: 0.916667\n",
            "Validation loss decreased (0.202724 --> 0.202701).  Saving model ...\n",
            "Epoch: 316 \tTraining Loss: 0.132165 \tValidation Loss: 0.203422 \tAccuracy: 0.916667\n",
            "Epoch: 317 \tTraining Loss: 0.107850 \tValidation Loss: 0.201824 \tAccuracy: 0.916667\n",
            "Validation loss decreased (0.202701 --> 0.201824).  Saving model ...\n",
            "Epoch: 318 \tTraining Loss: 0.118632 \tValidation Loss: 0.201628 \tAccuracy: 0.916667\n",
            "Validation loss decreased (0.201824 --> 0.201628).  Saving model ...\n",
            "Epoch: 319 \tTraining Loss: 0.119220 \tValidation Loss: 0.201883 \tAccuracy: 0.916667\n",
            "Epoch: 320 \tTraining Loss: 0.097963 \tValidation Loss: 0.200791 \tAccuracy: 0.916667\n",
            "Validation loss decreased (0.201628 --> 0.200791).  Saving model ...\n",
            "Epoch: 321 \tTraining Loss: 0.117482 \tValidation Loss: 0.201596 \tAccuracy: 0.916667\n",
            "Epoch: 322 \tTraining Loss: 0.089708 \tValidation Loss: 0.201903 \tAccuracy: 0.916667\n",
            "Epoch: 323 \tTraining Loss: 0.114065 \tValidation Loss: 0.202586 \tAccuracy: 0.916667\n",
            "Epoch: 324 \tTraining Loss: 0.111017 \tValidation Loss: 0.202599 \tAccuracy: 0.916667\n",
            "Epoch: 325 \tTraining Loss: 0.090104 \tValidation Loss: 0.203349 \tAccuracy: 0.916667\n",
            "Epoch: 326 \tTraining Loss: 0.114652 \tValidation Loss: 0.202789 \tAccuracy: 0.916667\n",
            "Epoch: 327 \tTraining Loss: 0.141236 \tValidation Loss: 0.202177 \tAccuracy: 0.916667\n",
            "Epoch: 328 \tTraining Loss: 0.114719 \tValidation Loss: 0.202006 \tAccuracy: 0.916667\n",
            "Epoch: 329 \tTraining Loss: 0.115174 \tValidation Loss: 0.202878 \tAccuracy: 0.916667\n",
            "Epoch: 330 \tTraining Loss: 0.157296 \tValidation Loss: 0.203724 \tAccuracy: 0.916667\n",
            "Epoch: 331 \tTraining Loss: 0.104301 \tValidation Loss: 0.204410 \tAccuracy: 0.916667\n",
            "Epoch: 332 \tTraining Loss: 0.098262 \tValidation Loss: 0.203481 \tAccuracy: 0.916667\n",
            "Epoch: 333 \tTraining Loss: 0.104749 \tValidation Loss: 0.203415 \tAccuracy: 0.916667\n",
            "Epoch: 334 \tTraining Loss: 0.098854 \tValidation Loss: 0.204818 \tAccuracy: 0.916667\n",
            "Epoch: 335 \tTraining Loss: 0.138677 \tValidation Loss: 0.204398 \tAccuracy: 0.916667\n",
            "Epoch: 336 \tTraining Loss: 0.097727 \tValidation Loss: 0.203833 \tAccuracy: 0.916667\n",
            "Epoch: 337 \tTraining Loss: 0.096936 \tValidation Loss: 0.205062 \tAccuracy: 0.916667\n",
            "Epoch: 338 \tTraining Loss: 0.160696 \tValidation Loss: 0.204392 \tAccuracy: 0.916667\n",
            "Epoch: 339 \tTraining Loss: 0.143320 \tValidation Loss: 0.203976 \tAccuracy: 0.916667\n",
            "Epoch: 340 \tTraining Loss: 0.109603 \tValidation Loss: 0.204412 \tAccuracy: 0.916667\n",
            "Epoch: 341 \tTraining Loss: 0.127542 \tValidation Loss: 0.204154 \tAccuracy: 0.916667\n",
            "Epoch: 342 \tTraining Loss: 0.121219 \tValidation Loss: 0.205082 \tAccuracy: 0.916667\n",
            "Epoch: 343 \tTraining Loss: 0.115744 \tValidation Loss: 0.204505 \tAccuracy: 0.916667\n",
            "Epoch: 344 \tTraining Loss: 0.130613 \tValidation Loss: 0.203941 \tAccuracy: 0.916667\n",
            "Epoch: 345 \tTraining Loss: 0.123808 \tValidation Loss: 0.203900 \tAccuracy: 0.916667\n",
            "Epoch: 346 \tTraining Loss: 0.085316 \tValidation Loss: 0.203776 \tAccuracy: 0.916667\n",
            "Epoch: 347 \tTraining Loss: 0.081930 \tValidation Loss: 0.203919 \tAccuracy: 0.916667\n",
            "Epoch: 348 \tTraining Loss: 0.096186 \tValidation Loss: 0.202849 \tAccuracy: 0.916667\n",
            "Epoch: 349 \tTraining Loss: 0.079367 \tValidation Loss: 0.201966 \tAccuracy: 0.916667\n",
            "Epoch: 350 \tTraining Loss: 0.102627 \tValidation Loss: 0.202430 \tAccuracy: 0.916667\n",
            "Epoch: 351 \tTraining Loss: 0.140719 \tValidation Loss: 0.202482 \tAccuracy: 0.916667\n",
            "Epoch: 352 \tTraining Loss: 0.119149 \tValidation Loss: 0.202943 \tAccuracy: 0.916667\n",
            "Epoch: 353 \tTraining Loss: 0.087153 \tValidation Loss: 0.202086 \tAccuracy: 0.916667\n",
            "Epoch: 354 \tTraining Loss: 0.078467 \tValidation Loss: 0.203245 \tAccuracy: 0.916667\n",
            "Epoch: 355 \tTraining Loss: 0.103378 \tValidation Loss: 0.203405 \tAccuracy: 0.916667\n",
            "Epoch: 356 \tTraining Loss: 0.110961 \tValidation Loss: 0.204019 \tAccuracy: 0.916667\n",
            "Epoch: 357 \tTraining Loss: 0.089608 \tValidation Loss: 0.203906 \tAccuracy: 0.916667\n",
            "Epoch: 358 \tTraining Loss: 0.150673 \tValidation Loss: 0.203871 \tAccuracy: 0.916667\n",
            "Epoch: 359 \tTraining Loss: 0.094981 \tValidation Loss: 0.203493 \tAccuracy: 0.916667\n",
            "Epoch: 360 \tTraining Loss: 0.091632 \tValidation Loss: 0.204333 \tAccuracy: 0.916667\n",
            "Epoch: 361 \tTraining Loss: 0.117002 \tValidation Loss: 0.204505 \tAccuracy: 0.916667\n",
            "Epoch: 362 \tTraining Loss: 0.112445 \tValidation Loss: 0.204196 \tAccuracy: 0.916667\n",
            "Epoch: 363 \tTraining Loss: 0.124043 \tValidation Loss: 0.204539 \tAccuracy: 0.916667\n",
            "Epoch: 364 \tTraining Loss: 0.150569 \tValidation Loss: 0.204729 \tAccuracy: 0.916667\n",
            "Epoch: 365 \tTraining Loss: 0.110238 \tValidation Loss: 0.204538 \tAccuracy: 0.916667\n",
            "Epoch: 366 \tTraining Loss: 0.090110 \tValidation Loss: 0.204541 \tAccuracy: 0.916667\n",
            "Epoch: 367 \tTraining Loss: 0.114378 \tValidation Loss: 0.205877 \tAccuracy: 0.916667\n",
            "Epoch: 368 \tTraining Loss: 0.096820 \tValidation Loss: 0.206121 \tAccuracy: 0.916667\n",
            "Epoch: 369 \tTraining Loss: 0.091702 \tValidation Loss: 0.206159 \tAccuracy: 0.916667\n",
            "Epoch: 370 \tTraining Loss: 0.121090 \tValidation Loss: 0.205431 \tAccuracy: 0.916667\n",
            "Epoch: 371 \tTraining Loss: 0.080250 \tValidation Loss: 0.205683 \tAccuracy: 0.916667\n",
            "Epoch: 372 \tTraining Loss: 0.084299 \tValidation Loss: 0.205931 \tAccuracy: 0.916667\n",
            "Epoch: 373 \tTraining Loss: 0.109383 \tValidation Loss: 0.205717 \tAccuracy: 0.916667\n",
            "Epoch: 374 \tTraining Loss: 0.098248 \tValidation Loss: 0.205290 \tAccuracy: 0.916667\n",
            "Epoch: 375 \tTraining Loss: 0.104336 \tValidation Loss: 0.205010 \tAccuracy: 0.916667\n",
            "Epoch: 376 \tTraining Loss: 0.088237 \tValidation Loss: 0.204716 \tAccuracy: 0.916667\n",
            "Epoch: 377 \tTraining Loss: 0.109893 \tValidation Loss: 0.204692 \tAccuracy: 0.916667\n",
            "Epoch: 378 \tTraining Loss: 0.125681 \tValidation Loss: 0.205297 \tAccuracy: 0.916667\n",
            "Epoch: 379 \tTraining Loss: 0.131926 \tValidation Loss: 0.204750 \tAccuracy: 0.916667\n",
            "Epoch: 380 \tTraining Loss: 0.096491 \tValidation Loss: 0.204977 \tAccuracy: 0.916667\n",
            "Epoch: 381 \tTraining Loss: 0.122417 \tValidation Loss: 0.205747 \tAccuracy: 0.916667\n",
            "Epoch: 382 \tTraining Loss: 0.111430 \tValidation Loss: 0.205836 \tAccuracy: 0.916667\n",
            "Epoch: 383 \tTraining Loss: 0.099318 \tValidation Loss: 0.206159 \tAccuracy: 0.916667\n",
            "Epoch: 384 \tTraining Loss: 0.125450 \tValidation Loss: 0.205436 \tAccuracy: 0.916667\n",
            "Epoch: 385 \tTraining Loss: 0.109540 \tValidation Loss: 0.205261 \tAccuracy: 0.916667\n",
            "Epoch: 386 \tTraining Loss: 0.107859 \tValidation Loss: 0.205021 \tAccuracy: 0.916667\n",
            "Epoch: 387 \tTraining Loss: 0.092389 \tValidation Loss: 0.204805 \tAccuracy: 0.916667\n",
            "Epoch: 388 \tTraining Loss: 0.137062 \tValidation Loss: 0.204478 \tAccuracy: 0.916667\n",
            "Epoch: 389 \tTraining Loss: 0.140346 \tValidation Loss: 0.204415 \tAccuracy: 0.916667\n",
            "Epoch: 390 \tTraining Loss: 0.103653 \tValidation Loss: 0.205005 \tAccuracy: 0.916667\n",
            "Epoch: 391 \tTraining Loss: 0.097644 \tValidation Loss: 0.205609 \tAccuracy: 0.916667\n",
            "Epoch: 392 \tTraining Loss: 0.124331 \tValidation Loss: 0.206389 \tAccuracy: 0.916667\n",
            "Epoch: 393 \tTraining Loss: 0.097049 \tValidation Loss: 0.205523 \tAccuracy: 0.916667\n",
            "Epoch: 394 \tTraining Loss: 0.100125 \tValidation Loss: 0.205190 \tAccuracy: 0.916667\n",
            "Epoch: 395 \tTraining Loss: 0.132125 \tValidation Loss: 0.204389 \tAccuracy: 0.916667\n",
            "Epoch: 396 \tTraining Loss: 0.125077 \tValidation Loss: 0.203579 \tAccuracy: 0.916667\n",
            "Epoch: 397 \tTraining Loss: 0.098233 \tValidation Loss: 0.204609 \tAccuracy: 0.916667\n",
            "Epoch: 398 \tTraining Loss: 0.100995 \tValidation Loss: 0.204868 \tAccuracy: 0.916667\n",
            "Epoch: 399 \tTraining Loss: 0.107497 \tValidation Loss: 0.204657 \tAccuracy: 0.916667\n",
            "Epoch: 400 \tTraining Loss: 0.094455 \tValidation Loss: 0.205859 \tAccuracy: 0.916667\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}